{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Setting foot into the world of Machine Learning with IRIS.\n\nThe Iris dataset was used in R.A. Fisher's classic 1936 paper, [The Use of Multiple Measurements in Taxonomic Problems](http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf), and can also be found on the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/).\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\n- Id\n- SepalLengthCm\n- SepalWidthCm\n- PetalLengthCm\n- PetalWidthCm\n- Species\n\n![](https://i.imgur.com/7iqseyn.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading the csv file and saving the data into a dataframe named df.\ndf = pd.read_csv('/kaggle/input/iris/Iris.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the first 5(by default) rows of the dataframe\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring the data with dataframe.describe()\n\nThe **describe()** method is used for calculating some statistical data like **percentile, mean and std** of the numerical values of the Series or DataFrame. It analyzes both numeric and object series and also the DataFrame column sets of mixed data types.\n\n**Syntax**\n> DataFrame.describe(percentiles=None, include=None, exclude=None)  "},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/NEGwo2c.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() # When given a mix of categorical and numerical data, then By default it describes only the attributes with numerical values. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include ='all') # include ='all' is a parameter used to include all the numerical as well as categorical values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the null values in the column\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count of each kind of flower species by creating a bar Plot using Pandas.\ndf['Species'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Designing the Pairplot\n\nThe pair plot uses two basic figures - The histogram and the scatter plot. The histogram on the diagonal allows us to visualize the distribution of single variable. While the scatter plot on the uper half and lower half traingles shows the relation between two variables. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove the Id column as logically it does not have much worth in prediting the Species. Right!\ndf = df[df.columns.drop('Id')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally plotting the pair plot using the seaborn library\nsns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysis of the above Pairplot: \n- First graph from the top row represents the count/frequncy distribution of SepalLengthCm\n- Second graph represents weak negative relationship between SepalLengthCm and SepalWidthCm\n- Third and fourth graph shows a strong positive relationship of SepalLengthCm with PetalLengthCm and PetalWidthCm"},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix\nA correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.\n\n### An example of a correlation matrix\nMostly, a correlation matrix is “square”, with the same variables shown in the rows and columns. The below visualization shows the correlations matrix. \n\n- It is used to state the importance of various things to people. \n- The line of 1.00s going from the top left to the bottom right is the main diagonal, which shows that each variable always perfectly correlates with itself. \n- This matrix is symmetrical. The correlation of elements above the diagonal are mirror image of the ones which are below.\n\n### Applications of a correlation matrix\n\nThere are three broad reasons for using a correlation matrix:\n\n- While summarizing a large amount of data where our goal is to see patterns in the data. \n- From our example above, now we can easily tell which of the variables highly correlate with each other and which don't.\n- To input into other analyses. For example, people commonly use correlation matrixes as inputs for:\n\n    - Exploratory factor analysis \n    - Confirmatory factor analysis\n    - Structural equation models \n    \n    \n- Used with linear regression, for example: A high amount of correlations suggests that the linear regression estimates will be unreliable. In other words, highly correlated data might represent similar information."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confirming the same by finding the correlation of all the relevant attributes.\ndf.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Better way to represent the correlation matrix\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first glance, it doesn’t seem like we’ll be able to figure out much from the heatmap. Well, that’s because this heatmap is in its most basic form. Let me walk you through the most common ways of formatting a correlation heatmap so that your data can be presented in a way that is both effective and visually appealing."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding Annotation - Displaying numbers inside the each cell of the heat map\nsns.heatmap(corr, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since upper half is mirror of the below half of the diagonal, you can remove either of them. \n# Generate a mask for the upper triangle.\nmask = np.triu(np.ones_like(corr, dtype=bool)) \n#np.triu is used for Upper triangle of an array.\n#np.ones_like: Return an array of ones with the same shape and type as a given array.\nsns.heatmap(corr, annot = True, mask=mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a mask for the lower triangle.\nmask = np.tril(np.ones_like(corr, dtype=bool)) #np.tril is used for Lower triangle of an array.\nsns.heatmap(corr, annot = True, mask=mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Invert Axis\nmask = np.triu(np.ones_like(corr, dtype=bool))\nax = sns.heatmap(corr, annot = True, mask=mask)\nax.invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pandas Profiling: EDA with single line command\n\nGenerates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n\n- Type inference: detect the types of columns in a dataframe.\n- Essentials: type, unique values, missing values\n- Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\n- Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n- Most frequent values\n- Histogram\n- Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n- Missing values matrix, count, heatmap and dendrogram of missing values\n- Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Machine Learning to Predict the Flower Species\n\n### Steps To Be followed When Applying an Algorithm\n- Split the dataset into training and testing dataset. The testing dataset is generally smaller than training one as it will help in training the model better.\n- Select any algorithm based on the problem (classification or regression) whatever you feel may be good.\n- Then pass the training dataset to the algorithm to train it. We use the .fit() method\n- Then pass the testing data to the trained algorithm to predict the outcome. We use the .predict() method.\n- We then check the accuracy by passing the predicted outcome and the actual output to the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test sets\nX = df.drop(['Species'],axis=1)\ny = df['Species']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n#Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n# fit the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# evaluate the model\nprediction = model.predict(X_test)\n\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nmodel = svm.SVC()\nmodel.fit(X_train,y_train) \nprediction=model.predict(X_test)\nprint('The accuracy of the SVM is:',metrics.accuracy_score(prediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nmodel=KNeighborsClassifier(n_neighbors=3)\nmodel.fit(X_train,y_train)\nprediction=model.predict(X_test)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier()\nmodel.fit(X_train,y_train) \nprediction=model.predict(X_test) \nprint('The accuracy of the Decision Tree is ',metrics.accuracy_score(prediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=20,criterion='entropy',random_state=0)\nmodel.fit(X_train,y_train) \nprediction=model.predict(X_test) \nprint('The accuracy of the Random Forest is ',metrics.accuracy_score(prediction,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"petal=df[['PetalLengthCm','PetalWidthCm','Species']]\nsepal=df[['SepalLengthCm','SepalWidthCm','Species']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_p,test_p=train_test_split(petal,test_size=0.3,random_state=0)  #petals\ntrain_x_p=train_p[['PetalWidthCm','PetalLengthCm']]\ntrain_y_p=train_p.Species\ntest_x_p=test_p[['PetalWidthCm','PetalLengthCm']]\ntest_y_p=test_p.Species","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_s,test_s=train_test_split(sepal,test_size=0.3,random_state=0)  #Sepal\ntrain_x_s=train_s[['SepalWidthCm','SepalLengthCm']]\ntrain_y_s=train_s.Species\ntest_x_s=test_s[['SepalWidthCm','SepalLengthCm']]\ntest_y_s=test_s.Species","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=svm.SVC()\nmodel.fit(train_x_p,train_y_p) \nprediction=model.predict(test_x_p) \nprint('The accuracy of the SVM using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n\nmodel=svm.SVC()\nmodel.fit(train_x_s,train_y_s) \nprediction=model.predict(test_x_s) \nprint('The accuracy of the SVM using Sepal is:',metrics.accuracy_score(prediction,test_y_s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_x_p,train_y_p) \nprediction=model.predict(test_x_p) \nprint('The accuracy of the Logistic Regression using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n\nmodel.fit(train_x_s,train_y_s) \nprediction=model.predict(test_x_s) \nprint('The accuracy of the Logistic Regression using Sepals is:',metrics.accuracy_score(prediction,test_y_s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_x_p,train_y_p) \nprediction=model.predict(test_x_p) \nprint('The accuracy of the Decision Tree using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n\nmodel.fit(train_x_s,train_y_s) \nprediction=model.predict(test_x_s) \nprint('The accuracy of the Decision Tree using Sepals is:',metrics.accuracy_score(prediction,test_y_s))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=KNeighborsClassifier(n_neighbors=3) \nmodel.fit(train_x_p,train_y_p) \nprediction=model.predict(test_x_p) \nprint('The accuracy of the KNN using Petals is:',metrics.accuracy_score(prediction,test_y_p))\n\nmodel.fit(train_x_s,train_y_s) \nprediction=model.predict(test_x_s) \nprint('The accuracy of the KNN using Sepals is:',metrics.accuracy_score(prediction,test_y_s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n\nUsing Petals over Sepal for training the data gives a much better accuracy.\nThis was expected as we saw in the heatmap above that the correlation between the Sepal Width and Length was very low whereas the correlation between Petal Width and Length was very high."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycaret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing pycaret classification method\n\nfrom pycaret.classification import *\n# This is the first step of model selection\n# Here the data is our datasets, target is the labeled \n# column(dependent variable), section is random number for future identification.\nexp = setup(data = df, target = 'Species', session_id=77 )\n\n# After this we will get a list of our columns and its type, just confirm they are the same. Then hit enter.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dataset\nfrom pycaret.datasets import get_data\ndiabetes = get_data('iris')\n\n# Importing module and initializing setup\nfrom pycaret.classification import *\nclf1 = setup(data = diabetes, target = 'species')\n\n# return best model\nbest = compare_models()\n\n# return top 3 models based on 'Accuracy'\ntop3 = compare_models(n_select = 3)\n\n# return best model based on AUC\nbest = compare_models(sort = 'AUC') #default is 'Accuracy'\n\n# compare specific models\n# best_specific = compare_models(whitelist = ['dt','rf','xgboost'])\n\n# blacklist certain models\n# best_specific = compare_models(blacklist = ['catboost', 'svm'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}